{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch to point extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an input the script should take the path of the STAC collection for Sentinel-1 and Sentinel-2 patch extractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the S1 and S2 STAC collections have to be split per EPSG, as openeo's `load_stac` can only handle one CRS in a STAC collection.\n",
    "This can be accomplished with GFMap's `split_collection_by_epsg`, as shown in the code block below. \n",
    "\n",
    "NOTE: In the finale pipeline the STAC collections will be on /vitodata/worldcereal_data. For example purposes, my own public folder is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openeo_gfmap.utils import split_collection_by_epsg\n",
    "\n",
    "# stac_path = '/data/users/Public/vincent.verelst/world_cereal/s1_s2_meteo_extraction/Sentinel2/stac/collection.json'\n",
    "# output_dir = '/data/users/Public/vincent.verelst/world_cereal/s1_s2_meteo_extraction/Sentinel2/small-split-stac/'\n",
    "\n",
    "# split_collection_by_epsg(stac_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the script will generate a list of all split STAC collections. As it isn't know up front how many different CRS were in the original STAC collection, it's not known into how many STAC collections it will be split.\n",
    "\n",
    "An assertion is added to make sure that the number of S1 and S2 STAC collections is the same. (As normally both S1 and S2 STAC collection should have the same spatiotemporal extent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the paths to the split collections\n",
    "import glob\n",
    "s1_collection_paths = glob.glob(\"/data/users/Public/vincent.verelst/world_cereal/s1_s2_meteo_extraction/Sentinel1/split_stac/*/collection.json\")\n",
    "s2_collection_paths = glob.glob(\"/data/users/Public/vincent.verelst/world_cereal/s1_s2_meteo_extraction/Sentinel2/small-split-stac/*/collection.json\")\n",
    "\n",
    "assert len(s1_collection_paths) == len(s2_collection_paths), \"Mismatch in number of collections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths are stored in ordered lists. Again (since lists are ordered) we have to make sure that the order of both S1 and S2 lists is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_order_collection(s1_collection_paths, s2_collection_paths):\n",
    "    for i in range(len(s1_collection_paths)):\n",
    "        s1_epsg = s1_collection_paths[i].split('/')[-2]\n",
    "        s2_epsg = s2_collection_paths[i].split('/')[-2]\n",
    "        assert s1_epsg == s2_epsg, f\"Order mismatch between {s1_epsg} and {s2_epsg}\"\n",
    "\n",
    "check_order_collection(s1_collection_paths, s2_collection_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can make a job dataframe which can be given as an input to the GFMapJobManager. We will create one batch job per UTM zone/CRS (to be seen if that is sufficiently small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>s1_collection_path</th>\n",
       "      <th>s2_collection_path</th>\n",
       "      <th>epsg</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>terrascope</td>\n",
       "      <td>2016-10-30</td>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>/data/users/Public/vincent.verelst/world_cerea...</td>\n",
       "      <td>/data/users/Public/vincent.verelst/world_cerea...</td>\n",
       "      <td>32736</td>\n",
       "      <td>[[34.453280157301236, -0.9129803702473933, 34....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  backend_name  start_date    end_date  \\\n",
       "0   terrascope  2016-10-30  2018-05-03   \n",
       "\n",
       "                                  s1_collection_path  \\\n",
       "0  /data/users/Public/vincent.verelst/world_cerea...   \n",
       "\n",
       "                                  s2_collection_path   epsg  \\\n",
       "0  /data/users/Public/vincent.verelst/world_cerea...  32736   \n",
       "\n",
       "                                            geometry  \n",
       "0  [[34.453280157301236, -0.9129803702473933, 34....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pystac\n",
    "from typing import List\n",
    "from openeo_gfmap import Backend\n",
    "\n",
    "def create_job_dataframe(backend: Backend, s1_collection_paths: List[str], s2_collection_paths: List[str]) -> pd.DataFrame:\n",
    "    columns = ['backend_name', 'start_date', 'end_date', 's1_collection_path', 's2_collection_path', 'epsg', 'geometry']\n",
    "    rows = []\n",
    "    for idx, collection_path in enumerate(s1_collection_paths):\n",
    "        coll = pystac.read_file(collection_path)\n",
    "        start_date = coll.extent.temporal.intervals[0][0]\n",
    "        end_date = coll.extent.temporal.intervals[0][1]\n",
    "        geometry = coll.extent.spatial.bboxes\n",
    "        epsg = collection_path.split('/')[-2][11:]\n",
    "\n",
    "        rows.append(\n",
    "            pd.Series(\n",
    "                dict(zip(columns, [backend.value, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), collection_path, s2_collection_paths[idx], epsg, geometry]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe(Backend.TERRASCOPE, s1_collection_paths, s2_collection_paths)\n",
    "\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDM API Interaction\n",
    "\n",
    "This is a placeholder to get a list of the URL to all geoparquets in the RDM intersection with the S1 and S2 STAC collections.  \n",
    "We can use `query_ground_truth` to create a GeoParquet file to a specific output folder with all points in the RDM that are within a specified Polygon. The Polygon will be constructed from the job_df geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create_datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the `create_datacube` function to be given to the GFMapJobManager is created. For this we cannot use worldcereal_classification's `worldcereal_preprocessed_inputs` in it's current form. Maybe best to discuss how we unify as much as possible the preprocessing of the different scripts.\n",
    "\n",
    "METEO is loaded from a pre-composited STAC collection --> we have to think about how we keep this up-to-date (maybe include a trigger in the automatic RDM API interaction?)\n",
    "\n",
    "In the final script we will include customizable job options, but we first need to figure out the optimal options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "import pandas as pd\n",
    "\n",
    "from openeo_gfmap import BackendContext, FetchType, TemporalContext\n",
    "from openeo_gfmap.spatial import BoundingBoxExtent\n",
    "from openeo_gfmap.preprocessing.compositing import median_compositing\n",
    "from worldcereal.openeo.preprocessing import raw_datacube_DEM\n",
    "\n",
    "def create_datacube(row: pd.Series, connection: openeo.Connection, provider=None, connection_provider=None):\n",
    "    bbox = row.geometry[0]\n",
    "    spatial_extent = BoundingBoxExtent(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    temporal_context = TemporalContext(row.start_date, row.end_date)  # In the end we'll probably don't need this. Leaving this in, just in case for now.\n",
    "\n",
    "    backend = Backend.TERRASCOPE\n",
    "    backend_context = BackendContext(backend)\n",
    "\n",
    "    fetch_type = FetchType.TILE\n",
    "\n",
    "    # The S1 and S2 cubes are loaded from their respective STAC collections\n",
    "    s1_cube = connection.load_stac(row.s1_collection_path, temporal_extent=[row.start_date, row.end_date])\n",
    "    s1_cube = median_compositing(s1_cube, period=\"month\")\n",
    "    \n",
    "    s2_bands = [\n",
    "            \"S2-L2A-B01\",\n",
    "            \"S2-L2A-B02\",\n",
    "            \"S2-L2A-B03\",\n",
    "            \"S2-L2A-B04\",\n",
    "            \"S2-L2A-B05\",\n",
    "            \"S2-L2A-B06\",\n",
    "            \"S2-L2A-B07\",\n",
    "            \"S2-L2A-B08\",\n",
    "            \"S2-L2A-B8A\",\n",
    "            \"S2-L2A-B11\",\n",
    "            \"S2-L2A-B12\"\n",
    "        ]  # Not extracting SCL and distance to cloud, can easily be added if needed\n",
    "    s2_cube = connection.load_stac(row.s2_collection_path, temporal_extent=[row.start_date, row.end_date])\n",
    "    s2_cube = s2_cube.filter_bands(s2_bands)\n",
    "    s2_cube = median_compositing(s2_cube, period=\"month\")\n",
    "\n",
    "    cube = s1_cube.merge_cubes(s2_cube)\n",
    "\n",
    "    # For DEM we can use worldcereal_classification's raw_datacube_DEM\n",
    "    dem_cube = raw_datacube_DEM(\n",
    "        connection=connection,\n",
    "        backend_context=backend_context,\n",
    "        spatial_extent=spatial_extent,\n",
    "        fetch_type=fetch_type,\n",
    "        )\n",
    "    dem_cube = dem_cube.resample_cube_spatial(s2_cube, method=\"bilinear\")\n",
    "    dem_cube = dem_cube.linear_scale_range(0, 65534, 0, 65534)  # Will not work for geoparquet output (openeo-devs are aware, but are not giving it priority)\n",
    "\n",
    "    # METEO is loaded from a pre-composited STAC collection\n",
    "    meteo_cube = connection.load_stac('/data/users/Public/vincent.verelst/world_cereal/agera5_monthly/stac/collection.json', spatial_extent=dict(spatial_extent), temporal_extent=[row.start_date, row.end_date])\n",
    "    meteo_cube.result_node().update_arguments(featureflags={\"tilesize\": 1})\n",
    "    meteo_cube = meteo_cube.rename_labels(\n",
    "            dimension=\"bands\", target=[\"AGERA5-PRECIP\", \"AGERA5-TMEAN\"]\n",
    "        )\n",
    "\n",
    "    cube = s1_cube.merge_cubes(s2_cube)\n",
    "    cube = cube.merge_cubes(dem_cube)\n",
    "    cube = cube.merge_cubes(meteo_cube)\n",
    "    \n",
    "    # This is just a placeholder for the RDM API interaction function. In this example we just load in some random points from parquet as an example.\n",
    "    geometries = connection.load_url('https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap/32736-random-points.geoparquet', format='Parquet')\n",
    "\n",
    "    cube = cube.aggregate_spatial(geometries, reducer='mean')\n",
    "\n",
    "    return cube.create_job(\n",
    "        title='Example patch to point in gfmap',\n",
    "        out_format='Parquet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function to generate an output path. This is just an example of a folder structure. In the end, the parquet that comes out of this pipeline will have to be merged to the global Worldcereal parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_path(output_dir, idx, row):\n",
    "    return output_dir / row.epsg / 'point_timeseries.geoparquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a post-job action to convert all values in the geoparquet values to uint16, as this isn't implemented in the OpenEO backend yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def post_job_action(\n",
    "    job_items: List[pystac.Item], row: pd.Series, parameters: Optional[dict] = None\n",
    ") -> list:\n",
    "    for idx, item in enumerate(job_items):\n",
    "        item_asset_path = Path(list(item.assets.values())[0].href)\n",
    "\n",
    "        gdf = gpd.read_parquet(item_asset_path)\n",
    "\n",
    "        # Convert the dates to datetime format\n",
    "        gdf[\"date\"] = pd.to_datetime(gdf[\"date\"])\n",
    "\n",
    "        # Convert band dtype to uint16 (temporary fix)\n",
    "        # TODO: remove this step when the issue is fixed on the OpenEO backend\n",
    "        bands = [\n",
    "            \"S2-L2A-B01\",\n",
    "            \"S2-L2A-B02\",\n",
    "            \"S2-L2A-B03\",\n",
    "            \"S2-L2A-B04\",\n",
    "            \"S2-L2A-B05\",\n",
    "            \"S2-L2A-B06\",\n",
    "            \"S2-L2A-B07\",\n",
    "            \"S2-L2A-B08\",\n",
    "            \"S2-L2A-B8A\",\n",
    "            \"S2-L2A-B11\",\n",
    "            \"S2-L2A-B12\",\n",
    "            \"S1-SIGMA0-VH\",\n",
    "            \"S1-SIGMA0-VV\",\n",
    "            \"elevation\",\n",
    "            \"AGERA5-PRECIP\",\n",
    "            \"AGERA5-TMEAN\",\n",
    "        ]\n",
    "        gdf[bands] = gdf[bands].fillna(65535).astype(\"uint16\")\n",
    "\n",
    "        gdf.to_parquet(item_asset_path, index=False)\n",
    "\n",
    "    return job_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the GFMapJobManager and let the job(s) run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.backend import vito_connection\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('/data/users/Public/vincent.verelst/world_cereal/test_patch_to_point_gfmap/')\n",
    "job_db = output_dir / 'job_db.parquet'\n",
    "\n",
    "job_manager = GFMAPJobManager(\n",
    "    output_dir=output_dir,\n",
    "    output_path_generator=generate_output_path,\n",
    "    post_job_action=post_job_action,\n",
    "    poll_sleep=60,\n",
    "    n_threads=1,\n",
    "    stac_enabled=False,  # Do we need a STAC collection for these geoparquet files?\n",
    ")\n",
    "\n",
    "job_manager.add_backend(\n",
    "    Backend.TERRASCOPE.value, vito_connection, parallel_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:55:06,337|openeo_gfmap.manager|INFO:  Starting ThreadPoolExecutor with 1 workers.\n",
      "2024-09-19 12:55:06,339|openeo_gfmap.manager|INFO:  Creating and running jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:00:42,486|openeo_gfmap.manager|INFO:  Job j-2409190c790144da9eba9dd28a9382a7 finished successfully, queueing on_job_done...\n",
      "2024-09-19 13:00:44,640|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-2409190c790144da9eba9dd28a9382a7\n",
      "2024-09-19 13:00:45,313|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2024-09-19 13:00:45,315|openeo_gfmap.manager|INFO:  Job j-2409190c790144da9eba9dd28a9382a7 and post job action finished successfully.\n",
      "2024-09-19 13:01:42,781|openeo_gfmap.manager|INFO:  Quitting job tracking & waiting for last post-job actions to finish.\n",
      "2024-09-19 13:01:42,783|openeo_gfmap.manager|INFO:  Exiting ThreadPoolExecutor.\n",
      "2024-09-19 13:01:42,786|openeo_gfmap.manager|INFO:  All jobs finished running.\n",
      "2024-09-19 13:01:42,788|openeo_gfmap.manager|INFO:  STAC was disabled, skipping generation of the catalogue.\n"
     ]
    }
   ],
   "source": [
    "job_manager.run_jobs(df=job_df, start_job=create_datacube, output_file=job_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>feature_index</th>\n",
       "      <th>S1-SIGMA0-VV</th>\n",
       "      <th>S1-SIGMA0-VH</th>\n",
       "      <th>S2-L2A-B01</th>\n",
       "      <th>S2-L2A-B02</th>\n",
       "      <th>S2-L2A-B03</th>\n",
       "      <th>S2-L2A-B04</th>\n",
       "      <th>S2-L2A-B05</th>\n",
       "      <th>S2-L2A-B06</th>\n",
       "      <th>S2-L2A-B07</th>\n",
       "      <th>S2-L2A-B08</th>\n",
       "      <th>S2-L2A-B8A</th>\n",
       "      <th>S2-L2A-B11</th>\n",
       "      <th>S2-L2A-B12</th>\n",
       "      <th>elevation</th>\n",
       "      <th>AGERA5-PRECIP</th>\n",
       "      <th>AGERA5-TMEAN</th>\n",
       "      <th>geometry</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1607</td>\n",
       "      <td>1292</td>\n",
       "      <td>1540</td>\n",
       "      <td>1578</td>\n",
       "      <td>2047</td>\n",
       "      <td>2687</td>\n",
       "      <td>2867</td>\n",
       "      <td>2884</td>\n",
       "      <td>5161</td>\n",
       "      <td>2441</td>\n",
       "      <td>1596</td>\n",
       "      <td>1376</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>POINT (34.76511 -0.37040)</td>\n",
       "      <td>openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1608</td>\n",
       "      <td>1496</td>\n",
       "      <td>1648</td>\n",
       "      <td>1672</td>\n",
       "      <td>2226</td>\n",
       "      <td>3148</td>\n",
       "      <td>3435</td>\n",
       "      <td>3428</td>\n",
       "      <td>3506</td>\n",
       "      <td>3051</td>\n",
       "      <td>1836</td>\n",
       "      <td>1530</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>POINT (34.79640 -0.47529)</td>\n",
       "      <td>openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-01 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>837</td>\n",
       "      <td>820</td>\n",
       "      <td>1050</td>\n",
       "      <td>1040</td>\n",
       "      <td>1511</td>\n",
       "      <td>2442</td>\n",
       "      <td>2718</td>\n",
       "      <td>2746</td>\n",
       "      <td>3151</td>\n",
       "      <td>2369</td>\n",
       "      <td>1680</td>\n",
       "      <td>1232</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>POINT (34.45666 -0.55460)</td>\n",
       "      <td>openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-01 00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>789</td>\n",
       "      <td>762</td>\n",
       "      <td>1052</td>\n",
       "      <td>802</td>\n",
       "      <td>1451</td>\n",
       "      <td>2965</td>\n",
       "      <td>3482</td>\n",
       "      <td>3372</td>\n",
       "      <td>3647</td>\n",
       "      <td>2445</td>\n",
       "      <td>1458</td>\n",
       "      <td>1336</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>POINT (34.45616 -0.67797)</td>\n",
       "      <td>openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-01 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>504</td>\n",
       "      <td>734</td>\n",
       "      <td>706</td>\n",
       "      <td>1128</td>\n",
       "      <td>2242</td>\n",
       "      <td>2597</td>\n",
       "      <td>2446</td>\n",
       "      <td>2778</td>\n",
       "      <td>1998</td>\n",
       "      <td>1280</td>\n",
       "      <td>1312</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>POINT (34.48585 -0.80910)</td>\n",
       "      <td>openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  feature_index  S1-SIGMA0-VV  S1-SIGMA0-VH  \\\n",
       "0 2016-11-01 00:00:00+00:00              0             0             0   \n",
       "1 2016-11-01 00:00:00+00:00              1             0             0   \n",
       "2 2016-11-01 00:00:00+00:00              2             0             0   \n",
       "3 2016-11-01 00:00:00+00:00              3             0             0   \n",
       "4 2016-11-01 00:00:00+00:00              4             0             0   \n",
       "\n",
       "   S2-L2A-B01  S2-L2A-B02  S2-L2A-B03  S2-L2A-B04  S2-L2A-B05  S2-L2A-B06  \\\n",
       "0        1607        1292        1540        1578        2047        2687   \n",
       "1        1608        1496        1648        1672        2226        3148   \n",
       "2         837         820        1050        1040        1511        2442   \n",
       "3         789         762        1052         802        1451        2965   \n",
       "4         475         504         734         706        1128        2242   \n",
       "\n",
       "   S2-L2A-B07  S2-L2A-B08  S2-L2A-B8A  S2-L2A-B11  S2-L2A-B12  elevation  \\\n",
       "0        2867        2884        5161        2441        1596       1376   \n",
       "1        3435        3428        3506        3051        1836       1530   \n",
       "2        2718        2746        3151        2369        1680       1232   \n",
       "3        3482        3372        3647        2445        1458       1336   \n",
       "4        2597        2446        2778        1998        1280       1312   \n",
       "\n",
       "   AGERA5-PRECIP  AGERA5-TMEAN                   geometry  \\\n",
       "0          65535         65535  POINT (34.76511 -0.37040)   \n",
       "1          65535         65535  POINT (34.79640 -0.47529)   \n",
       "2          65535         65535  POINT (34.45666 -0.55460)   \n",
       "3          65535         65535  POINT (34.45616 -0.67797)   \n",
       "4          65535         65535  POINT (34.48585 -0.80910)   \n",
       "\n",
       "                                                  id  \n",
       "0  openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...  \n",
       "1  openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...  \n",
       "2  openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...  \n",
       "3  openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...  \n",
       "4  openEO_2017_AF_One-Acre-Fund-MEL_POINT_110_201...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "results = gpd.read_parquet('/data/users/Public/vincent.verelst/world_cereal/test_patch_to_point_gfmap/32736/point_timeseries.geoparquet')\n",
    "results.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worldcereal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
