{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes_df = pd.read_excel('../.noteook-tests/Overview-reference-data-phase-1-20231222.xlsx', sheet_name=None)\n",
    "dataframes_df = dataframes_df['Overview']\n",
    "dataframes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that are ready to be extracted and uploaded to RDM\n",
    "to_process_dfs = dataframes_df[dataframes_df['Geoparquet Status'] == 'ready']['Name_Ewoc_II'].tolist()\n",
    "len(to_process_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to treat group0 separately, as extractions for it have already been done\n",
    "# here, we get all datasets that are part of group0\n",
    "import geopandas as gpd\n",
    "\n",
    "group_0_path = \"/vitodata/worldcereal_data/EXTRACTIONS/all_datasets/grouped_datasets/group_0.geoparquet\"\n",
    "group0_df = gpd.read_parquet(group_0_path)\n",
    "group0_datasets = group0_df['ref_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirected getting the datasets from the RDM folder instead of the excel file\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_paths = []\n",
    "for dataset in to_process_dfs:\n",
    "    part_dirs = glob.glob(str(rdm_dir / dataset / 'harmonized' / '*'))\n",
    "    part_dirs = [xx for xx in part_dirs if Path(xx).is_dir()]\n",
    "\n",
    "    if len(part_dirs) == 0:\n",
    "        try:\n",
    "            dataset_paths.append(glob.glob(str(rdm_dir / dataset / 'harmonized' / '*.geoparquet'))[0])\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        dataset_paths.extend(glob.glob(str(rdm_dir / dataset / 'harmonized' / '*' / '*.geoparquet')))\n",
    "\n",
    "# exclude group0 datasets\n",
    "dataset_paths = [xx for xx in dataset_paths if Path(xx).stem not in group0_datasets]\n",
    "to_process_dfs = [xx.split('/')[-1].split('.')[0] for xx in dataset_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# is this really needed as a separate run?\n",
    "\n",
    "# geometry_stats = {}\n",
    "# for df_name, path in tqdm(zip(to_process_dfs, dataset_paths), total=len(to_process_dfs)):\n",
    "#     df = gpd.read_parquet(path)\n",
    "    \n",
    "#     df[\"geometry\"] = df.geometry.centroid\n",
    "#     median_latitude = abs(df['geometry'].y).median()    \n",
    "#     geometry_stats[df_name] = median_latitude\n",
    "\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "\n",
    "# geometry_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reference dataset to a new parquet file\n",
    "dest_folder = Path('/vitodata/worldcereal_data/EXTRACTIONS/all_datasets/all_datasets_updated_flag/')\n",
    "\n",
    "def clear_write_dataset(ref_df, current_dataset_name, geometry_stats, dest_folder):\n",
    "    # added to handle the case where the dataset is a part of a larger dataset\n",
    "    parted_dataset_name = current_dataset_name\n",
    "    if \"_part\" in current_dataset_name:\n",
    "        current_dataset_name = current_dataset_name.split(\"_part\")[0]\n",
    "    ref_df['ref_id'] = current_dataset_name\n",
    "\n",
    "    if \" h3_l3_cell\" in ref_df.columns:\n",
    "        print(\"Rename column h3_l3_cell for dataset: \", current_dataset_name)\n",
    "        ref_df = ref_df.rename(columns={\" h3_l3_cell\": \"h3_l3_cell\"})\n",
    "\n",
    "    required_columns_and_types = [\n",
    "        ('ref_id', 'str'),\n",
    "        ('sample_id', 'str'),\n",
    "        ('h3_l3_cell', 'str'),\n",
    "        ('valid_time', 'str'),\n",
    "        ('extract', 'int32'),\n",
    "        ('ewoc_code', 'int64')\n",
    "    ]\n",
    "\n",
    "    for column, dtype in required_columns_and_types:\n",
    "        if column not in ref_df.columns:\n",
    "            raise ValueError(f'Column {column} not found in {current_dataset_name}')\n",
    "        if ref_df[column].dtype != dtype:\n",
    "            # since pandas does not have a dtype str, we check for object\n",
    "            if (dtype == \"str\") and (ref_df[column].dtype != \"O\"):\n",
    "                warnings.warn(f'Column {column} has dtype {ref_df[column].dtype} but should be {dtype}')\n",
    "                try:\n",
    "                    ref_df[column] = ref_df[column].astype(dtype)\n",
    "                except:\n",
    "                    print(f\"Exception when trying to convert column {column} from dataset {current_dataset_name} to {dtype}\")\n",
    "                    return\n",
    "\n",
    "    # moved getting median latitude to here instead of in the seperate run above\n",
    "    median_latitude = str(int(round(abs(ref_df.geometry.centroid.y).median()))).zfill(2)\n",
    "\n",
    "    dest_file = dest_folder / f'{median_latitude}_{parted_dataset_name}.geoparquet'\n",
    "    ref_df.to_parquet(dest_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_dataset_path, current_dataset_name in tqdm(zip(dataset_paths, to_process_dfs), total=len(dataset_paths)):\n",
    "    if not Path(current_dataset_path).exists():\n",
    "        raise FileNotFoundError(f'Couldn\\'t find {current_dataset_name} in path: {current_dataset_path}')\n",
    "    try:\n",
    "        ref_df = gpd.read_parquet(current_dataset_path)\n",
    "    except Exception:\n",
    "        raise IOError(f'Failed to read {current_dataset_path}')\n",
    "\n",
    "    clear_write_dataset(ref_df, current_dataset_name, geometry_stats, dest_folder)\n",
    "\n",
    "    del ref_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
